{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMDB LSTM Model Training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOeqDYo0poydsuYvl3k8stg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXsV6vZA-dRc","executionInfo":{"status":"ok","timestamp":1651836971889,"user_tz":-60,"elapsed":19833,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}},"outputId":"6f250a29-f4f6-4d15-ef09-59be58b1bc06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","#specify project directory in drive eg /content/drive/NLUProject\n","# drive.flush_and_unmount()\n","drive.mount('/content/drive')\n","\n","#define necessary imports\n","import time\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","import gensim"]},{"cell_type":"code","source":["# define the dataset \n","class IMBDDataset(Dataset):\n","    \"\"\"IMDB dataset.\"\"\"\n","\n","    # initialize the data in the dataset, pad the sequences to the max length\n","    def __init__(self, df):\n","        self.df = df\n","        self.maxLenForDF = self.getMaximumLengthSequence()\n","        self.padReturningItems(self.maxLenForDF)\n","        self.sendListToTensors()\n","\n","# pad all of the sequences up to a given length\n","    def padReturningItems(self, lengthToPadTo):\n","        for index, row in self.df.iterrows():\n","            # if(index % 100 == 0):\n","            #   print(index)\n","            paddingNeeded = (lengthToPadTo - len(row['review_tokens_reduced']))\n","            if(paddingNeeded > 0):\n","                padData = [paddingID] * paddingNeeded\n","                self.df.at[index, \"review_tokens_reduced\"] = row['review_tokens_reduced'] + padData\n","# convert data that will go to the model into tensors\n","    def sendListToTensors(self):\n","        for index, row in self.df.iterrows():\n","            self.df.at[index, \"review_tokens_reduced\"] = torch.tensor(row['review_tokens_reduced'], dtype=torch.int)\n","            self.df.at[index, \"user_id_transformed_to_label\"] = torch.tensor(row['user_id_transformed_to_label'], dtype=torch.int)\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def getMaximumLengthSequence(self):\n","        dfColumnAsList = self.df['review_tokens_reduced'].tolist()\n","        listOfListLengths = [len(i) for i in dfColumnAsList]\n","        return max(listOfListLengths)\n","# on getitem, return the row\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        dfRowToReturn = self.df.iloc[idx]\n","        dictToReturn = {'input': dfRowToReturn['review_tokens_reduced'], 'label': dfRowToReturn['user_id_transformed_to_label']}\n","        #print(dictToReturn)\n","        return dictToReturn\n","#get the number of unique authors\n","    def getAuthorCount(self):\n","        uniqueAuthors = self.df[\"user_id_transformed_to_label\"].unique()\n","        uniqueAuthorLength = len(uniqueAuthors)\n","        return uniqueAuthorLength\n"],"metadata":{"id":"xQMclxesKMBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load in the datasets\n","train_dataset = torch.load('drive/MyDrive/NLU Project/data/PyTorchDataset/IMDBtrain.pt')\n","validate_dataset = torch.load('drive/MyDrive/NLU Project/data/PyTorchDataset/IMDBvalidate.pt')\n","test_dataset = torch.load('drive/MyDrive/NLU Project/data/PyTorchDataset/IMDBtest.pt')"],"metadata":{"id":"5li16ouiJOsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# place the datasets in the dataloader\n","\n","batchSize = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batchSize)"],"metadata":{"id":"OXNirXy6JQLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define the model\n","class LSTMModel(torch.nn.Module):\n","\n","  def __init__(self, pretrainedEmbeddingWeights, lstmHiddenDimensions, lstmLayerNumber, outputClassNumber, paddingId):\n","    super(LSTMModel, self).__init__()\n","    #embedding layer converts tokens to the saved embedding weights\n","    self.EmbeddingLayer = nn.Embedding.from_pretrained(pretrainedEmbeddingWeights, padding_idx = paddingId)\n","    #get the embedding dimensins\n","    self.EmbeddingDimensions = pretrainedEmbeddingWeights.size()[1]\n","    # define the lstm\n","    self.LSTMLayer = nn.LSTM(self.EmbeddingDimensions, lstmHiddenDimensions, batch_first=True)\n","    #define the linear layer that reduces or expands hidden dimension to artist number\n","    self.LinearLayer = nn.Linear(lstmHiddenDimensions, outputClassNumber)\n","    # define softmax layer\n","    self.nnSoftMax = nn.LogSoftmax()\n","\n","  def forward(self, input):\n","    embeddedInput = self.EmbeddingLayer(input)\n","    lstmOutput, (finalHiddenStates, finalCellStates) = self.LSTMLayer(embeddedInput)\n","    #take the hidden state of the final layer\n","    #print(finalHiddenStates.size())\n","    # linearOutput = self.LinearLayer(finalHiddenStates[-1])\n","\n","    # seqLength = lstmOutput.shape[1]\n","    # print(lstmOutput.size())\n","    # outVector = torch.nn.functional.avg_pool2d(lstmOutput, kernel_size=(seqLength,1))\n","    lstmOutsPooled = torch.mean(lstmOutput, dim=1)\n","    # print(outVector.size())\n","    # print(outVector)\n","\n","    linearOutput = self.LinearLayer(lstmOutsPooled)\n","\n","    softMaxOut = self.nnSoftMax(linearOutput)\n","    # print(softMaxOut.size())\n","    return softMaxOut\n"],"metadata":{"id":"skicTv8pLIRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define function to check accuracy with a dataloader\n","def get_accuracy(dataloader, model):\n","\n","  model.eval()\n","  correctlyPredictedNum = 0\n","  acummulateLength = 0\n","\n","  for i, data in enumerate(dataloader):\n","\n","    modelOutputOnSampleBatch = model(data['input'])\n","    # print(modelOutputOnSampleBatch)\n","    classPredictions = np.argmax(modelOutputOnSampleBatch.detach().numpy(), axis=1)\n","    # print(classPredictions, \"predictions\")\n","    # print(data['label'], \"actual\")\n","    correctBoolean = classPredictions == data['label'].detach().numpy()\n","    # print(correctBoolean)\n","    correctlyPredictedNum += np.sum(correctBoolean)\n","\n","    acummulateLength+= data['input'].size(0)\n","    # print(correctlyPredictedNum)\n","    # print(\"accum\", correctlyPredictedNum)\n","\n","    # print(\"Size of modelOutput\", modelOutputOnSampleBatch.size())\n","    # print(\"Size of labels\", data['label'].size())\n","    # print(\"Size of argmax\", len(classPredictions))\n","  # print(correctlyPredictedNum)\n","  # print( len(dataloader))\n","  accuracyToReturn = correctlyPredictedNum / acummulateLength\n","  model.train()\n","\n","  return accuracyToReturn"],"metadata":{"id":"XlcMfJ3zKtax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define tokens that relate to helping with embeddings generation for the model\n","unknownToken = \"<unk>\"\n","padToken = \"<pad>\"\n","# load pretrained embeddings\n","model = gensim.models.KeyedVectors.load('drive/MyDrive/NLU Project/data/preProcessedEmbeddings/glove_vectors.kv')\n","\n","paddingID = model.vocab[padToken].index"],"metadata":{"id":"CSHYp6U2LYN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputSize = train_dataset.getAuthorCount()"],"metadata":{"id":"fxOWC8gpPKMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define hyperparameters\n","hiddnSize = 64\n","layersLstm = 3\n","\n","\n","# get the weights to be placed in embedding layer\n","preTrainedEmbeddings = torch.from_numpy(model.vectors)\n","# define the model\n","validationModel = LSTMModel(preTrainedEmbeddings, hiddnSize, layersLstm, outputSize, paddingID)\n","# convert values to flaot so all layers are consistent\n","validationModel.float()\n","# define loss function \n","lossFunction = nn.NLLLoss()\n","#define optimizer including the LR hyperparamter\n","optimizer = torch.optim.Adam(validationModel.parameters(), lr=0.001)\n","\n","#define lists and variables for training tracking\n","epochTestAccuracyList = []\n","epochTrainAccuracyList = []\n","\n","lossForBatch = 0\n","import time\n","timeStart = time.time()\n","\n","#train across epoch encounterings of the data\n","for epoch in range(20):\n","  #break if the epoch number is not met within a given time\n","  if(time.time() - timeStart > 43200):\n","      break\n","  for i, data in enumerate(train_dataloader):\n","    if(time.time() - timeStart > 43200):\n","      break\n","    # set the model to expect to be trained\n","    validationModel.train()\n","    # clear the gradient calculations from last backward\n","    optimizer.zero_grad()\n","    # run the model on the input\n","    modelOutputOnSampleBatch = validationModel(data['input'])\n","    # calculate the loss between output and expected result\n","    lossForBatch = lossFunction(modelOutputOnSampleBatch, data['label'])\n","    # calculate the gradient\n","    lossForBatch.backward()\n","    # update the model weights\n","    optimizer.step()\n","    if(i % 100 == 0):\n","      print(\"Step run on batch\", i, \"time:\",(time.time() - timeStart), \"loss:\", lossForBatch)\n","\n","  testAccuracyThisEpoch = get_accuracy(test_dataloader, validationModel)\n","  print(\"Accuracy epoch TESTING\", epoch, \":\", testAccuracyThisEpoch, \"time:\", (time.time() - timeStart))\n","  epochTestAccuracyList.append(testAccuracyThisEpoch)\n","\n","  trainAccuracyThisEpoch = get_accuracy(train_dataloader, validationModel)\n","  print(\"Accuracy epoch TRAINING\", epoch, \":\", trainAccuracyThisEpoch, \"time:\", (time.time() - timeStart))\n","  epochTrainAccuracyList.append(trainAccuracyThisEpoch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"id":"RxA0AP2yKzjP","executionInfo":{"status":"error","timestamp":1651838812677,"user_tz":-60,"elapsed":270342,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}},"outputId":"7f1f8e80-eba9-490a-958e-f3ad6eb416b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["Step run on batch 0 time: 2.5277223587036133 loss: tensor(4.1382, grad_fn=<NllLossBackward0>)\n","Step run on batch 100 time: 221.509920835495 loss: tensor(3.9171, grad_fn=<NllLossBackward0>)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-f9fa1d914b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodelOutputOnSampleBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlossForBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelOutputOnSampleBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlossForBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# # save the lists\n","# import pickle\n","# with open('drive/MyDrive/NLU Project/data/LSTMTestTrainingList', 'wb') as f:\n","#   pickle.dump(epochTestAccuracyList, f)\n","# with open('drive/MyDrive/NLU Project/data/LSTMTrainTrainingList', 'wb') as f:\n","#   pickle.dump(epochTrainAccuracyList, f)"],"metadata":{"id":"kQeffr39MTaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # save the model weights\n","# torch.save(validationModel.state_dict(), 'drive/MyDrive/NLU Project/models/IMDBLSTMmodel')\n"],"metadata":{"id":"TL2d5tO8Mcod"},"execution_count":null,"outputs":[]}]}