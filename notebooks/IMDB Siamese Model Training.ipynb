{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMDB Siamese Model Training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSMZd6GqelZlLYdTZV+52c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"htowREtg_xaP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652009577387,"user_tz":-60,"elapsed":3117,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}},"outputId":"b66a7b69-3d7f-4f1f-ff33-f9db897e55eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","#specify project directory in drive eg /content/drive/NLUProject\n","# drive.flush_and_unmount()\n","drive.mount('/content/drive')\n","\n","#define necessary imports\n","import time\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","import gensim"]},{"cell_type":"code","source":["# Siamese Dataset class definition\n","\n","class IMBDSiameseDataset(Dataset):\n","    \"\"\"IMDB dataset for siamese implementation.\"\"\"\n","\n","    def __init__(self, df):\n","        self.df = df\n","        self.maxLenForDF = self.getMaximumLengthSequence()\n","        self.padReturningItems(self.maxLenForDF)\n","        self.sendListToTensors()\n","\n","    def padReturningItems(self, lengthToPadTo):\n","        for index, row in self.df.iterrows():\n","            paddingNeeded = (lengthToPadTo - len(row['review_tokens_reduced']))\n","            if(paddingNeeded > 0):\n","                padData = [paddingID] * paddingNeeded\n","                self.df.at[index, \"review_tokens_reduced\"] = row['review_tokens_reduced'] + padData\n","\n","    def sendListToTensors(self):\n","        for index, row in self.df.iterrows():\n","            self.df.at[index, \"review_tokens_reduced\"] = torch.tensor(row['review_tokens_reduced'], dtype=torch.int)\n","            self.df.at[index, \"user_id_transformed_to_label\"] = torch.tensor(row['user_id_transformed_to_label'], dtype=torch.int)\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def getMaximumLengthSequence(self):\n","        dfColumnAsList = self.df['review_tokens_reduced'].tolist()\n","        listOfListLengths = [len(i) for i in dfColumnAsList]\n","        return max(listOfListLengths)\n","# on getitem, 50% of the time take another random item that is of the same author, and 50% of the time take a different author's document\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        input1 = self.df.sample()\n","\n","        should_get_same_class = np.random.randint(0,2) \n","        if should_get_same_class:\n","            while True:\n","                input2 = self.df.sample()\n","                if input1['user_id_transformed_to_label'].iloc[0] == input2['user_id_transformed_to_label'].iloc[0]:\n","                    break\n","        else:\n","\n","            while True:\n","                input2 = self.df.sample()\n","                if input1['user_id_transformed_to_label'].iloc[0] != input2['user_id_transformed_to_label'].iloc[0]:\n","                    break\n","\n","\n","        dictToReturn = {'input1': input1['review_tokens_reduced'].iloc[0], 'label1': input1['user_id_transformed_to_label'].iloc[0], \n","                        'input2': input2['review_tokens_reduced'].iloc[0], 'label2': input2['user_id_transformed_to_label'].iloc[0], \n","                        'same_class' : int(input1['user_id_transformed_to_label'].iloc[0] == input2['user_id_transformed_to_label'].iloc[0])}\n","        return dictToReturn\n","        \n","    def getAuthorCount(self):\n","        uniqueAuthors = self.df[\"user_id_transformed_to_label\"].unique()\n","        uniqueAuthorLength = len(uniqueAuthors)\n","        return uniqueAuthorLength\n"],"metadata":{"id":"JEOaCWQ7aa4N","executionInfo":{"status":"ok","timestamp":1652009577387,"user_tz":-60,"elapsed":6,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#load in the datasets\n","siamese_train_dataset = torch.load('drive/MyDrive/NLU Project/data/PyTorchDataset/IMDBtrainSiamese.pt')\n","siamese_validate_dataset = torch.load('drive/MyDrive/NLU Project/data/PyTorchDataset/IMDBvalidateSiamese.pt')\n","siamese_test_dataset = torch.load('drive/MyDrive/NLU Project/data/PyTorchDataset/IMDBtestSiamese.pt')"],"metadata":{"id":"guDKLdXCaevJ","executionInfo":{"status":"ok","timestamp":1652009596904,"user_tz":-60,"elapsed":19523,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#define dataloaders\n","batchSize = 64\n","siamese_train_dataloader = DataLoader(siamese_train_dataset, batch_size=batchSize, shuffle=True)\n","siamese_test_dataloader = DataLoader(siamese_test_dataset, batch_size=batchSize, shuffle=True)"],"metadata":{"id":"FsYmCKeDan6k","executionInfo":{"status":"ok","timestamp":1652009596904,"user_tz":-60,"elapsed":5,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Define GRU model\n","class SiameseModel(torch.nn.Module):\n","\n","  def __init__(self, pretrainedEmbeddingWeights, gruHiddenDimensions, gruLayerNumber, outputClassNumber, paddingId):\n","    super(SiameseModel, self).__init__()\n","    self.EmbeddingLayer = nn.Embedding.from_pretrained(pretrainedEmbeddingWeights, padding_idx = paddingId)\n","    self.EmbeddingDimensions = pretrainedEmbeddingWeights.size()[1]\n","    self.GRULayer = nn.GRU(self.EmbeddingDimensions, gruHiddenDimensions, batch_first=True)\n","    self.LinearLayer = nn.Linear(gruHiddenDimensions, outputClassNumber)\n","    self.nnSoftMax = nn.LogSoftmax()\n","  #network forward passes a single input through the single network as shown by the forward of previous GRU model\n","  def network_forward(self, input):\n","    embeddedInput = self.EmbeddingLayer(input)\n","    gruOutput, finalHiddenStates = self.GRULayer(embeddedInput)\n","    gruOutsPooled = torch.mean(gruOutput, dim=1)\n","    linearOutput = self.LinearLayer(gruOutsPooled)\n","\n","    softMaxOut = self.nnSoftMax(linearOutput)\n","    return softMaxOut\n","#return the outputs of both inputs through the model\n","  def forward(self, input1, input2):\n","    output1 = self.network_forward(input1)\n","    output2 = self.network_forward(input2)\n","\n","    return output1, output2\n","# get the loss of the model on an input with its labels\n","  def getLoss(self, input1, input2, label, real1, real2):\n","    #forward on both inputs\n","    output1 = self.network_forward(input1)\n","    output2 = self.network_forward(input2)\n","#get the negative log likelihood of both inputs for loss\n","    out1ModelPredictionLoss = torch.nn.functional.nll_loss(output1, real1)\n","    out2ModelPredictionLoss = torch.nn.functional.nll_loss(output2, real2)\n","# get the consine similarity between the two outputs\n","    similarityMeasures = torch.nn.functional.cosine_similarity(output1, output2, dim=1)\n","#for batches, take the mean difference in expected similarity and the actual similarity\n","    similarityLoss = torch.mean(((1-label) * similarityMeasures) +          #if they are different, we want no similarity, so use the measure as loss\n","                                ((label) * torch.absolute(torch.sub(similarityMeasures, 1))))            #if they are the same we want the similarity to be 1, so add loss as |result-1|\n","\n","\n","    #increase the 0-1 range of simLoss to 0-5\n","    normalizedLoss = 5*(similarityLoss)\n","#combine all the loss values and return\n","    fullLoss = out1ModelPredictionLoss + out2ModelPredictionLoss + normalizedLoss\n","    return fullLoss\n","\n","      "],"metadata":{"id":"6JAZUlPlawJM","executionInfo":{"status":"ok","timestamp":1652009596904,"user_tz":-60,"elapsed":4,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#load pretrained embeddings\n","model = gensim.models.KeyedVectors.load('drive/MyDrive/NLU Project/data/preProcessedEmbeddings/glove_vectors.kv')\n","unknownToken = \"<unk>\"\n","padToken = \"<pad>\"\n","paddingID = model.vocab[padToken].index\n","preTrainedEmbeddings = torch.from_numpy(model.vectors)"],"metadata":{"id":"RQ5AbNEPbVcv","executionInfo":{"status":"ok","timestamp":1652009598515,"user_tz":-60,"elapsed":1615,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#define hyperparameters\n","hiddnSize = 64\n","layersGru = 3\n","outputSizeSiamese = siamese_test_dataset.getAuthorCount()"],"metadata":{"id":"zCPrmcfrbLJ0","executionInfo":{"status":"ok","timestamp":1652009598515,"user_tz":-60,"elapsed":4,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Train siamese model\n","\n","siameseValidationModel = SiameseModel(preTrainedEmbeddings, hiddnSize, layersGru, outputSizeSiamese, paddingID)\n","\n","siameseValidationModel.float()\n","optimizer = torch.optim.Adam(siameseValidationModel.parameters(), lr = 0.002)\n","\n","\n","epochTestAccuracyList = []\n","epochTrainAccuracyList = []\n","\n","lossForBatch = 0\n","import time\n","timeStart = time.time()\n","for epoch in range(50):\n","  if(time.time() - timeStart > 39600):\n","      break\n","  for i, data in enumerate(siamese_train_dataloader):\n","    if(time.time() - timeStart > 39600):\n","      break\n","    siameseValidationModel.train()\n","    optimizer.zero_grad()\n","\n","    # Pass the outputs of the networks and label into the loss function\n","    combinedLoss = siameseValidationModel.getLoss(data['input1'], data['input2'], data['same_class'], data['label1'], data['label2'])\n","\n","    # Calculate the backpropagation\n","    combinedLoss.backward()\n","    \n","    optimizer.step()\n","\n","    if i % 25 == 0 :\n","      print(f\"Epoch number {epoch}\\n Current loss {combinedLoss.item()}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"TiMJ5Awvayz-","executionInfo":{"status":"error","timestamp":1652009648929,"user_tz":-60,"elapsed":50417,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}},"outputId":"23bc493b-1014-4714-fe9f-1371d6033c0b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch number 0\n"," Current loss 11.473751068115234\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4f92f5eeea79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimeStart\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m39600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_train_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimeStart\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m39600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-1aebd1b90d0e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_get_same_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0minput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id_transformed_to_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id_transformed_to_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5365\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5366\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5368\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m         new_data = self._mgr.take(\n\u001b[0;32m-> 3616\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3617\u001b[0m         )\n\u001b[1;32m   3618\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"take\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    868\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         )\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m    686\u001b[0m                     ),\n\u001b[1;32m    687\u001b[0m                 )\n\u001b[0;32m--> 688\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m             ]\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    686\u001b[0m                     ),\n\u001b[1;32m    687\u001b[0m                 )\n\u001b[0;32m--> 688\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m             ]\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# #save model weights\n","# torch.save(siameseValidationModel.state_dict(), 'drive/MyDrive/NLU Project/data/modelSiameseIMDB')"],"metadata":{"id":"TvE1_ow4a4VG","executionInfo":{"status":"aborted","timestamp":1652009648928,"user_tz":-60,"elapsed":3,"user":{"displayName":"Harvey South","userId":"00286034947533783714"}}},"execution_count":null,"outputs":[]}]}